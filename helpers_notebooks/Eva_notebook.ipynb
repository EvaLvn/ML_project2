{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1edaa86d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e18d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pre_processing import *\n",
    "from textblob import TextBlob\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a1b578b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 're' from '/Users/robinjaccard/opt/anaconda3/lib/python3.8/re.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a4c976c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>lang</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>linked</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>withheld_in_countries</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>withheld_copyright</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-26 10:48:35+00:00</td>\n",
       "      <td>1365252415444946945</td>\n",
       "      <td>1365252415444946944</td>\n",
       "      <td>#Balakot \\nPak Army is our pride ‚ù§Ô∏èüëç https://t...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>en</td>\n",
       "      <td>2021-02-26 10:48:35.662</td>\n",
       "      <td>no</td>\n",
       "      <td>[0, 35]</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>{'media': [{'id': 1365252409015033857, 'id_str...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-26 10:45:51+00:00</td>\n",
       "      <td>1365251727574900738</td>\n",
       "      <td>1365251727574900736</td>\n",
       "      <td>RT @ZaidZamanHamid: ŸÑŸà⁄ØŸà⁄∫ ⁄©€å ÿß⁄©ÿ´ÿ±€åÿ™ €ÅÿØÿß€åÿ™ ŸÜ€Å€å⁄∫...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ur</td>\n",
       "      <td>2021-02-26 10:45:51.661</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Thu Feb 25 18:59:12 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-26 10:45:51+00:00</td>\n",
       "      <td>1365251727574900738</td>\n",
       "      <td>1365251727574900736</td>\n",
       "      <td>RT @ZaidZamanHamid: ŸÑŸà⁄ØŸà⁄∫ ⁄©€å ÿß⁄©ÿ´ÿ±€åÿ™ €ÅÿØÿß€åÿ™ ŸÜ€Å€å⁄∫...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ur</td>\n",
       "      <td>2021-02-26 10:45:51.661</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Thu Feb 25 18:59:12 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-26 10:38:57+00:00</td>\n",
       "      <td>1365249991137251328</td>\n",
       "      <td>1365249991137251328</td>\n",
       "      <td>RT @SaniaNishtar: ÿ≥€åÿØ ÿßÿ®ÿ±ÿß€Å€åŸÖ ⁄©ÿß ÿ™ÿπŸÑŸÇ ŸÇÿ®ÿßÿ¶ŸÑ€å ÿ∂...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ur</td>\n",
       "      <td>2021-02-26 10:38:57.662</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Fri Feb 26 05:23:00 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-02-26 10:10:45+00:00</td>\n",
       "      <td>1365242894362279938</td>\n",
       "      <td>1365242894362279936</td>\n",
       "      <td>RT @mosa_abumarzook: ŸÅŸä ŸÖÿ´ŸÑ ŸÅÿ¨ÿ± Ÿáÿ∞ÿß ÿßŸÑŸäŸàŸÖ ŸÇÿ®ŸÑ ...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ar</td>\n",
       "      <td>2021-02-26 10:10:45.659</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IL]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Thu Feb 25 19:04:40 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-02-26 10:10:45+00:00</td>\n",
       "      <td>1365242894362279938</td>\n",
       "      <td>1365242894362279936</td>\n",
       "      <td>RT @mosa_abumarzook: ŸÅŸä ŸÖÿ´ŸÑ ŸÅÿ¨ÿ± Ÿáÿ∞ÿß ÿßŸÑŸäŸàŸÖ ŸÇÿ®ŸÑ ...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ar</td>\n",
       "      <td>2021-02-26 10:10:45.659</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IL]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Thu Feb 25 19:04:40 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-02-26 10:10:46+00:00</td>\n",
       "      <td>1365242898569134080</td>\n",
       "      <td>1365242898569134080</td>\n",
       "      <td>RT @Saimhun: Mujhe aj bhi batting ki bari ni d...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>hi</td>\n",
       "      <td>2021-02-26 10:10:46.662</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>{'media': [{'id': 1365238103892516864, 'id_str...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'created_at': 'Fri Feb 26 09:51:44 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-02-26 10:10:46+00:00</td>\n",
       "      <td>1365242898569134080</td>\n",
       "      <td>1365242898569134080</td>\n",
       "      <td>RT @Saimhun: Mujhe aj bhi batting ki bari ni d...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>hi</td>\n",
       "      <td>2021-02-26 10:10:46.662</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>{'media': [{'id': 1365238103892516864, 'id_str...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'created_at': 'Fri Feb 26 09:51:44 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-02-26 10:59:24+00:00</td>\n",
       "      <td>1365255137552457730</td>\n",
       "      <td>1365255137552457728</td>\n",
       "      <td>RT @iVeenaKhan: (ÿß€í Ÿæ€åÿ∫ŸÖÿ®ÿ±Ô∑∫! ŸÑŸà⁄ØŸà⁄∫ ÿ≥€í) ⁄©€Å€Å ÿØŸà ...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ur</td>\n",
       "      <td>2021-02-26 10:59:24.663</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Fri Feb 26 05:42:25 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-02-26 10:59:24+00:00</td>\n",
       "      <td>1365255137552457730</td>\n",
       "      <td>1365255137552457728</td>\n",
       "      <td>RT @iVeenaKhan: (ÿß€í Ÿæ€åÿ∫ŸÖÿ®ÿ±Ô∑∫! ŸÑŸà⁄ØŸà⁄∫ ÿ≥€í) ⁄©€Å€Å ÿØŸà ...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>low</td>\n",
       "      <td>ur</td>\n",
       "      <td>2021-02-26 10:59:24.663</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'created_at': 'Fri Feb 26 05:42:25 +0000 2021...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at                   id               id_str  \\\n",
       "1  2021-02-26 10:48:35+00:00  1365252415444946945  1365252415444946944   \n",
       "2  2021-02-26 10:45:51+00:00  1365251727574900738  1365251727574900736   \n",
       "3  2021-02-26 10:45:51+00:00  1365251727574900738  1365251727574900736   \n",
       "4  2021-02-26 10:38:57+00:00  1365249991137251328  1365249991137251328   \n",
       "5  2021-02-26 10:10:45+00:00  1365242894362279938  1365242894362279936   \n",
       "6  2021-02-26 10:10:45+00:00  1365242894362279938  1365242894362279936   \n",
       "7  2021-02-26 10:10:46+00:00  1365242898569134080  1365242898569134080   \n",
       "8  2021-02-26 10:10:46+00:00  1365242898569134080  1365242898569134080   \n",
       "9  2021-02-26 10:59:24+00:00  1365255137552457730  1365255137552457728   \n",
       "10 2021-02-26 10:59:24+00:00  1365255137552457730  1365255137552457728   \n",
       "\n",
       "                                                 text  \\\n",
       "1   #Balakot \\nPak Army is our pride ‚ù§Ô∏èüëç https://t...   \n",
       "2   RT @ZaidZamanHamid: ŸÑŸà⁄ØŸà⁄∫ ⁄©€å ÿß⁄©ÿ´ÿ±€åÿ™ €ÅÿØÿß€åÿ™ ŸÜ€Å€å⁄∫...   \n",
       "3   RT @ZaidZamanHamid: ŸÑŸà⁄ØŸà⁄∫ ⁄©€å ÿß⁄©ÿ´ÿ±€åÿ™ €ÅÿØÿß€åÿ™ ŸÜ€Å€å⁄∫...   \n",
       "4   RT @SaniaNishtar: ÿ≥€åÿØ ÿßÿ®ÿ±ÿß€Å€åŸÖ ⁄©ÿß ÿ™ÿπŸÑŸÇ ŸÇÿ®ÿßÿ¶ŸÑ€å ÿ∂...   \n",
       "5   RT @mosa_abumarzook: ŸÅŸä ŸÖÿ´ŸÑ ŸÅÿ¨ÿ± Ÿáÿ∞ÿß ÿßŸÑŸäŸàŸÖ ŸÇÿ®ŸÑ ...   \n",
       "6   RT @mosa_abumarzook: ŸÅŸä ŸÖÿ´ŸÑ ŸÅÿ¨ÿ± Ÿáÿ∞ÿß ÿßŸÑŸäŸàŸÖ ŸÇÿ®ŸÑ ...   \n",
       "7   RT @Saimhun: Mujhe aj bhi batting ki bari ni d...   \n",
       "8   RT @Saimhun: Mujhe aj bhi batting ki bari ni d...   \n",
       "9   RT @iVeenaKhan: (ÿß€í Ÿæ€åÿ∫ŸÖÿ®ÿ±Ô∑∫! ŸÑŸà⁄ØŸà⁄∫ ÿ≥€í) ⁄©€Å€Å ÿØŸà ...   \n",
       "10  RT @iVeenaKhan: (ÿß€í Ÿæ€åÿ∫ŸÖÿ®ÿ±Ô∑∫! ŸÑŸà⁄ØŸà⁄∫ ÿ≥€í) ⁄©€Å€Å ÿØŸà ...   \n",
       "\n",
       "                                               source  truncated  \\\n",
       "1   <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "2   <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "3   <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "4   <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "5   <a href=\"https://mobile.twitter.com\" rel=\"nofo...      False   \n",
       "6   <a href=\"https://mobile.twitter.com\" rel=\"nofo...      False   \n",
       "7   <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "8   <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "9   <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "10  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "\n",
       "    in_reply_to_status_id  in_reply_to_status_id_str  in_reply_to_user_id  \\\n",
       "1                     NaN                        NaN                  NaN   \n",
       "2                     NaN                        NaN                  NaN   \n",
       "3                     NaN                        NaN                  NaN   \n",
       "4                     NaN                        NaN                  NaN   \n",
       "5                     NaN                        NaN                  NaN   \n",
       "6                     NaN                        NaN                  NaN   \n",
       "7                     NaN                        NaN                  NaN   \n",
       "8                     NaN                        NaN                  NaN   \n",
       "9                     NaN                        NaN                  NaN   \n",
       "10                    NaN                        NaN                  NaN   \n",
       "\n",
       "    in_reply_to_user_id_str  ... filter_level lang            timestamp_ms  \\\n",
       "1                       NaN  ...          low   en 2021-02-26 10:48:35.662   \n",
       "2                       NaN  ...          low   ur 2021-02-26 10:45:51.661   \n",
       "3                       NaN  ...          low   ur 2021-02-26 10:45:51.661   \n",
       "4                       NaN  ...          low   ur 2021-02-26 10:38:57.662   \n",
       "5                       NaN  ...          low   ar 2021-02-26 10:10:45.659   \n",
       "6                       NaN  ...          low   ar 2021-02-26 10:10:45.659   \n",
       "7                       NaN  ...          low   hi 2021-02-26 10:10:46.662   \n",
       "8                       NaN  ...          low   hi 2021-02-26 10:10:46.662   \n",
       "9                       NaN  ...          low   ur 2021-02-26 10:59:24.663   \n",
       "10                      NaN  ...          low   ur 2021-02-26 10:59:24.663   \n",
       "\n",
       "       linked display_text_range  withheld_in_countries  \\\n",
       "1          no            [0, 35]                   [IN]   \n",
       "2          no                NaN                   [IN]   \n",
       "3   retweeted                NaN                   [IN]   \n",
       "4          no                NaN                   [IN]   \n",
       "5          no                NaN                   [IL]   \n",
       "6   retweeted                NaN                   [IL]   \n",
       "7          no                NaN                   [IN]   \n",
       "8   retweeted                NaN                   [IN]   \n",
       "9          no                NaN                   [IN]   \n",
       "10  retweeted                NaN                   [IN]   \n",
       "\n",
       "                                    extended_entities  possibly_sensitive  \\\n",
       "1   {'media': [{'id': 1365252409015033857, 'id_str...                 0.0   \n",
       "2                                                 NaN                 NaN   \n",
       "3                                                 NaN                 NaN   \n",
       "4                                                 NaN                 NaN   \n",
       "5                                                 NaN                 NaN   \n",
       "6                                                 NaN                 NaN   \n",
       "7   {'media': [{'id': 1365238103892516864, 'id_str...                 0.0   \n",
       "8   {'media': [{'id': 1365238103892516864, 'id_str...                 0.0   \n",
       "9                                                 NaN                 NaN   \n",
       "10                                                NaN                 NaN   \n",
       "\n",
       "                                     retweeted_status withheld_copyright  \n",
       "1                                                 NaN                NaN  \n",
       "2   {'created_at': 'Thu Feb 25 18:59:12 +0000 2021...                NaN  \n",
       "3   {'created_at': 'Thu Feb 25 18:59:12 +0000 2021...                NaN  \n",
       "4   {'created_at': 'Fri Feb 26 05:23:00 +0000 2021...                NaN  \n",
       "5   {'created_at': 'Thu Feb 25 19:04:40 +0000 2021...                NaN  \n",
       "6   {'created_at': 'Thu Feb 25 19:04:40 +0000 2021...                NaN  \n",
       "7   {'created_at': 'Fri Feb 26 09:51:44 +0000 2021...                NaN  \n",
       "8   {'created_at': 'Fri Feb 26 09:51:44 +0000 2021...                NaN  \n",
       "9   {'created_at': 'Fri Feb 26 05:42:25 +0000 2021...                NaN  \n",
       "10  {'created_at': 'Fri Feb 26 05:42:25 +0000 2021...                NaN  \n",
       "\n",
       "[10 rows x 39 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for r, d, f in os.walk(os.getcwd()):\n",
    "    for file in f:\n",
    "        if 'withheldtweets.json' in file:\n",
    "            dfs.append(pd.read_json(\"./censored_tweets/%s\" % file, lines=True))\n",
    "\n",
    "df_cen = pd.concat(dfs)\n",
    "df_cen = df_cen.dropna(subset=['withheld_in_countries'])\n",
    "df_cen.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b7d2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500\n",
    "pd.options.display.width = 300\n",
    "df_english = df_cen[df_cen['lang'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a656bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en', 'ur', 'ar', 'hi', 'und', 'cs', 'tr', 'in', 'de', 'da', 'es',\n",
       "       'ht', 'fr', 'tl', 'hu', 'uk', 'sd', 'bn', 'my', 'nl', 'et', 'ro',\n",
       "       'pa', 'fa', 'th', 'ja', 'fi', 'el', 'pt', 'it', 'zh', 'vi', 'ckb',\n",
       "       'ca', 'pl', 'sv', 'lt', 'cy', 'lv', 'km', 'ko', 'mr', 'ru', 'lo',\n",
       "       'ps', 'sl', 'ta', 'no', 'is', 'eu'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_languages = df_cen['lang'].unique()\n",
    "list_of_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bdb7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_traduction(dataframe):\n",
    "    list_en=[]\n",
    "    texts = list(dataframe['text'])\n",
    "    langs = list(dataframe['lang'])\n",
    "    for s in dataframe.shape[0]:\n",
    "        if langs[s] != 'und':\n",
    "            result = translator.translate(texts[s], \"english\", langs[s])\n",
    "            list_en.append(result)\n",
    "    \n",
    "    #df_new = dataframe[dataframe['lang'] != 'und']\n",
    "    #df_new['Translation'] = list_en\n",
    "    return pd.DataFrame(list_en, columns = 'translation')\n",
    "    #df_new.to_csv(\".\\EnglishTranslation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2274aa03",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4z/k7wkc9t97453h4tqprj_1j3h0000gn/T/ipykernel_17337/3244822482.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_gogogo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_csv_traduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4z/k7wkc9t97453h4tqprj_1j3h0000gn/T/ipykernel_17337/1956480889.py\u001b[0m in \u001b[0;36mcreate_csv_traduction\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlangs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lang'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlangs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'und'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "df_test = df_cen.head(100)\n",
    "df_gogogo = create_csv_traduction(df_test)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_text(df_text):\n",
    "    return df_text.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_cen.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc40ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_test = lower_text(df_cen['text'])\n",
    "lower_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cen[df_cen['lang']=='und']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199dc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04ebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae99a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spelling_correction(tweets):\n",
    "    textBlb = TextBlob(tweets)           \n",
    "    return textBlb.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_char(tweets):\n",
    "    return re.sub(r\"[^a-zA-Z0-9# ]\", \" \", tweets)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "    \n",
    "\n",
    "def clean_tweets_after_trad(df_text):\n",
    "    clean_tweets = df_text.apply(lambda x: remove_char(x))\n",
    "    clean_tweets = pd.DataFrame([ele for ele in clean_tweets if ele != ''], columns =['text'])['text']\n",
    "    clean_tweets = clean_tweets.apply(lambda x: x.lower())\n",
    "    #clean_tweets = clean_tweets.apply(lambda x: spelling_correction(x))\n",
    "    clean_tweets = clean_tweets.apply(lambda x : lemmatize_text(x))\n",
    "    clean_tweets = clean_tweets.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "def lemmatize_text(text):\n",
    "    return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b010eb",
   "metadata": {},
   "source": [
    "### Pour Robin ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f2aa913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/robinjaccard/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/robinjaccard/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/robinjaccard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1607c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_char(tweets):\n",
    "    return re.sub(r\"[^a-zA-Z0-9# ]\", \" \", tweets)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "    \n",
    "\n",
    "def clean_tweets_after_trad(df_text):\n",
    "    clean_tweets = df_text.apply(lambda x: remove_char(x))\n",
    "    clean_tweets = pd.DataFrame([ele for ele in clean_tweets if ele != ''], columns =['text'])['text']\n",
    "    clean_tweets = clean_tweets.apply(lambda x: x.lower())\n",
    "    clean_tweets = clean_tweets.apply(lambda x : lemmatize_text(x))\n",
    "    clean_tweets = clean_tweets.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43784120",
   "metadata": {},
   "source": [
    "### Fin ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3249d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b445a80f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4z/k7wkc9t97453h4tqprj_1j3h0000gn/T/ipykernel_17337/3818139400.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_txt_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_txt_clean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_en' is not defined"
     ]
    }
   ],
   "source": [
    "df_txt_clean = clean_tweets(df_en['text'])\n",
    "df_txt_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e8b9713",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_txt_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4z/k7wkc9t97453h4tqprj_1j3h0000gn/T/ipykernel_17337/345813645.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_txt_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_txt_clean' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(df_txt_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eb9dad0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_txt_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4z/k7wkc9t97453h4tqprj_1j3h0000gn/T/ipykernel_17337/1934990564.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_clean_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_tweets_after_trad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_txt_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_clean_after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_txt_clean' is not defined"
     ]
    }
   ],
   "source": [
    "df_clean_after = clean_tweets_after_trad(pd.DataFrame(df_txt_clean)['text'])\n",
    "df_clean_after"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac8f77b",
   "metadata": {},
   "source": [
    "# BTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd7dc90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitermplus as btm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "096ffaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_df = pd.read_csv('./out_clean.csv')\n",
    "my_df = pd.read_csv('data/to_be_clustered.csv.gz', compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a760dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['clean'].apply(lambda x: str(x))\n",
    "in_df = my_df[my_df['whcs'] == \"France\"]\n",
    "in_df = in_df['clean'].dropna()\n",
    "texts = in_df.str.strip().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eeea5d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "# Obtaining terms frequency in a sparse matrix and corpus vocabulary\n",
    "X, vocabulary, vocab_dict = btm.get_words_freqs(texts)\n",
    "tf = np.array(X.sum(axis=0)).ravel()\n",
    "# Vectorizing documents\n",
    "docs_vec = btm.get_vectorized_docs(texts, vocabulary)\n",
    "docs_lens = list(map(len, docs_vec))\n",
    "# Generating biterms\n",
    "biterms = btm.get_biterms(docs_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1cd73a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 164.72it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [00:00<00:00, 76155.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZING AND RUNNING MODEL\n",
    "#model = btm.BTM(\n",
    "#    X, vocabulary, seed=12321, T=8, M=20, alpha=50/8, beta=0.01)\n",
    "# 1000, 8, 20, 10/8, 0,1\n",
    "model = btm.BTM(\n",
    "    X, vocabulary, seed=1000, T=14, M=20, alpha=0.0000001, beta=1)\n",
    "model.fit(biterms, iterations=20)\n",
    "p_zd = model.transform(docs_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "83d9acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS\n",
    "perplexity = btm.perplexity(model.matrix_topics_words_, p_zd, X, 8)\n",
    "coherence = btm.coherence(model.matrix_topics_words_, X, M=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "01883ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1690.4946391552608"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "293b1ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-404.7659609 , -525.53980897, -149.78715713, -149.78715713,\n",
       "       -408.18676594, -455.25539157, -113.17284947, -448.41461985,\n",
       "       -149.78715713, -149.78715713, -442.32133113, -122.20608224,\n",
       "         12.44954718, -352.47143298])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3f1ecd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.06584605e-02, 1.99347051e-01, 7.37404624e-13, ...,\n",
       "        3.08804406e-03, 5.26521302e-03, 6.45287780e-03],\n",
       "       [3.12527155e-02, 7.68696192e-01, 1.09165738e-12, ...,\n",
       "        3.99293533e-03, 3.40799693e-03, 8.01507073e-03],\n",
       "       [8.15735265e-03, 9.40009085e-01, 1.84487538e-13, ...,\n",
       "        1.22331443e-03, 5.86392106e-04, 8.25481000e-03],\n",
       "       ...,\n",
       "       [1.94339063e-02, 7.57982537e-01, 6.72439448e-13, ...,\n",
       "        3.86848822e-03, 1.28638713e-03, 2.79507724e-03],\n",
       "       [1.67547310e-02, 7.61056628e-01, 7.51236642e-13, ...,\n",
       "        4.39769270e-03, 1.43712739e-03, 4.79204675e-03],\n",
       "       [1.40318605e-02, 2.57851966e-01, 2.39437854e-12, ...,\n",
       "        3.41467667e-02, 4.58048344e-03, 2.70811355e-02]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_zd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d041c5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>always</td>\n",
       "      <td>white</td>\n",
       "      <td>zomato</td>\n",
       "      <td>zomato</td>\n",
       "      <td>woman</td>\n",
       "      <td>muslim</td>\n",
       "      <td>courage</td>\n",
       "      <td>great</td>\n",
       "      <td>zomato</td>\n",
       "      <td>zomato</td>\n",
       "      <td>amp</td>\n",
       "      <td>nigga</td>\n",
       "      <td>wilder</td>\n",
       "      <td>cpac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anyone</td>\n",
       "      <td>people</td>\n",
       "      <td>fight</td>\n",
       "      <td>fight</td>\n",
       "      <td>amp</td>\n",
       "      <td>year</td>\n",
       "      <td>union</td>\n",
       "      <td>even</td>\n",
       "      <td>fight</td>\n",
       "      <td>fight</td>\n",
       "      <td>left</td>\n",
       "      <td>lil</td>\n",
       "      <td>maharashtra</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>know</td>\n",
       "      <td>get</td>\n",
       "      <td>fire</td>\n",
       "      <td>fire</td>\n",
       "      <td>sharia</td>\n",
       "      <td>new</td>\n",
       "      <td>major</td>\n",
       "      <td>soral</td>\n",
       "      <td>fire</td>\n",
       "      <td>fire</td>\n",
       "      <td>biden</td>\n",
       "      <td>poverty</td>\n",
       "      <td>shivgaan</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>communism</td>\n",
       "      <td>black</td>\n",
       "      <td>fine</td>\n",
       "      <td>fine</td>\n",
       "      <td>let</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>catherine</td>\n",
       "      <td>one</td>\n",
       "      <td>fine</td>\n",
       "      <td>fine</td>\n",
       "      <td>attack</td>\n",
       "      <td>percent</td>\n",
       "      <td>birth</td>\n",
       "      <td>lincoln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>national</td>\n",
       "      <td>say</td>\n",
       "      <td>find</td>\n",
       "      <td>find</td>\n",
       "      <td>hijab</td>\n",
       "      <td>migrant</td>\n",
       "      <td>something</td>\n",
       "      <td>someone</td>\n",
       "      <td>find</td>\n",
       "      <td>find</td>\n",
       "      <td>wing</td>\n",
       "      <td>shame</td>\n",
       "      <td>maharaj</td>\n",
       "      <td>president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>want</td>\n",
       "      <td>anti</td>\n",
       "      <td>finally</td>\n",
       "      <td>finally</td>\n",
       "      <td>islamic</td>\n",
       "      <td>twitter</td>\n",
       "      <td>anyrtgingh</td>\n",
       "      <td>big</td>\n",
       "      <td>finally</td>\n",
       "      <td>finally</td>\n",
       "      <td>video</td>\n",
       "      <td>andrew</td>\n",
       "      <td>round</td>\n",
       "      <td>republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>social</td>\n",
       "      <td>one</td>\n",
       "      <td>final</td>\n",
       "      <td>final</td>\n",
       "      <td>cair</td>\n",
       "      <td>attack</td>\n",
       "      <td>salutation</td>\n",
       "      <td>world</td>\n",
       "      <td>final</td>\n",
       "      <td>final</td>\n",
       "      <td>warn</td>\n",
       "      <td>release</td>\n",
       "      <td>shivaji</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>firearm</td>\n",
       "      <td>go</td>\n",
       "      <td>filth</td>\n",
       "      <td>filth</td>\n",
       "      <td>one</td>\n",
       "      <td>france</td>\n",
       "      <td>infect</td>\n",
       "      <td>reset</td>\n",
       "      <td>filth</td>\n",
       "      <td>filth</td>\n",
       "      <td>border</td>\n",
       "      <td>bail</td>\n",
       "      <td>umbrella</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>come</td>\n",
       "      <td>like</td>\n",
       "      <td>film</td>\n",
       "      <td>film</td>\n",
       "      <td>state</td>\n",
       "      <td>illegal</td>\n",
       "      <td>visit</td>\n",
       "      <td>play</td>\n",
       "      <td>film</td>\n",
       "      <td>film</td>\n",
       "      <td>break</td>\n",
       "      <td>rise</td>\n",
       "      <td>dutch</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>major</td>\n",
       "      <td>want</td>\n",
       "      <td>file</td>\n",
       "      <td>file</td>\n",
       "      <td>omar</td>\n",
       "      <td>school</td>\n",
       "      <td>music</td>\n",
       "      <td>alain</td>\n",
       "      <td>file</td>\n",
       "      <td>file</td>\n",
       "      <td>germany</td>\n",
       "      <td>hut</td>\n",
       "      <td>across</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic0  topic1   topic2   topic3   topic4     topic5      topic6   topic7   topic8   topic9  topic10  topic11      topic12     topic13\n",
       "0     always   white   zomato   zomato    woman     muslim     courage    great   zomato   zomato      amp    nigga       wilder        cpac\n",
       "1     anyone  people    fight    fight      amp       year       union     even    fight    fight     left      lil  maharashtra       trump\n",
       "2       know     get     fire     fire   sharia        new       major    soral     fire     fire    biden  poverty     shivgaan        must\n",
       "3  communism   black     fine     fine      let  terrorist   catherine      one     fine     fine   attack  percent        birth     lincoln\n",
       "4   national     say     find     find    hijab    migrant   something  someone     find     find     wing    shame      maharaj   president\n",
       "5       want    anti  finally  finally  islamic    twitter  anyrtgingh      big  finally  finally    video   andrew        round  republican\n",
       "6     social     one    final    final     cair     attack  salutation    world    final    final     warn  release      shivaji     project\n",
       "7    firearm      go    filth    filth      one     france      infect    reset    filth    filth   border     bail     umbrella        show\n",
       "8       come    like     film     film    state    illegal       visit     play     film     film    break     rise        dutch         yes\n",
       "9      major    want     file     file     omar     school       music    alain     file     file  germany      hut       across    national"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words = btm.get_top_topic_words(model, words_num=10)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f93a104",
   "metadata": {},
   "source": [
    "### Topics \n",
    "We can give categories we used for labelling to these clusters\n",
    "1. 'white/black - racism', 'women' 0\n",
    "1. 'jew - antisemtism' 2\n",
    "1. 'tweeter acccounts', 'communism - against bankers , against state  prder ' 3\n",
    "1. 'immigration' 4\n",
    "1. 'farright', 'Alain Soral', 'communism - against bankers , against state  prder ' 5\n",
    "1. 'islam', 'terrorism' 6\n",
    "1. 'american elections' 7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. 'homophobia'\n",
    "\n",
    "1. Other\n",
    "\n",
    "Don't have \n",
    "- 'India - Pakistan'\n",
    "- 'coronavirus - vaccination'\n",
    "- 'fantasy sex play'\n",
    "- 'homophobia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "535a7f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robinjaccard/opt/anaconda3/lib/python3.8/site-packages/tmplot/_helpers.py:38: UserWarning: Please install \"tomotopy\" package to analyze its models.\n",
      "Run `pip install tomotopy` in the console.\n",
      "  warn(\n",
      "/Users/robinjaccard/opt/anaconda3/lib/python3.8/site-packages/tmplot/_helpers.py:38: UserWarning: Please install \"tomotopy\" package to analyze its models.\n",
      "Run `pip install tomotopy` in the console.\n",
      "  warn(\n",
      "/Users/robinjaccard/opt/anaconda3/lib/python3.8/site-packages/tmplot/_helpers.py:38: UserWarning: Please install \"tomotopy\" package to analyze its models.\n",
      "Run `pip install tomotopy` in the console.\n",
      "  warn(\n",
      "/Users/robinjaccard/opt/anaconda3/lib/python3.8/site-packages/tmplot/_helpers.py:38: UserWarning: Please install \"tomotopy\" package to analyze its models.\n",
      "Run `pip install tomotopy` in the console.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beae3ead85d541c8a478869f4a216b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HBox(children=(HTML(value='<b>Select a topic</b>:'), Dropdown(options=((0, 0), (‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RESULTS VISUALIZATION\n",
    "# You need to install tmplot first\n",
    "import tmplot as tmp\n",
    "tmp.report(model=model, docs=texts,  show_headers = True,show_docs= False,show_topics= False, height=500, width= 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d28328fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  1,  1,  1,  5,  1,  1,  4,  1,  1,  1,  1,  0, 10,  5, 10,  1,\n",
       "        1, 10,  1,  1,  5,  7,  0,  0,  1,  0,  1, 10,  4,  7,  1,  1,  1,\n",
       "        1,  7,  5,  5, 10,  1,  7,  7, 10,  4, 10,  0,  1, 10,  7,  5, 10,\n",
       "        1,  1, 13,  1, 10,  1, 10,  5,  1,  1,  1,  7,  1, 10,  0,  1,  1,\n",
       "        1,  5,  5,  5,  7,  1,  1,  5,  1,  5,  1,  1, 11,  1,  7,  5,  4,\n",
       "        1,  5,  1,  1,  7,  1, 10, 11,  4,  0,  0, 10, 10, 10,  1,  0,  7,\n",
       "        1,  1,  7,  1,  1,  1,  1,  1, 10, 10,  1,  1, 10,  0,  1,  7,  1,\n",
       "       13,  1,  5,  1, 10,  7,  5,  1,  1,  7,  1,  4,  5,  1,  1,  7,  1,\n",
       "        0,  4,  0, 11,  0, 10,  0,  1,  1,  7,  1,  1,  1,  1,  1,  5,  1,\n",
       "       13, 10,  1,  1,  1,  1,  0,  5,  1,  1,  1,  1,  0,  0, 13, 13, 10,\n",
       "        7,  1,  1,  1, 10,  1, 10,  4, 10,  7, 10,  1,  1,  1, 10,  7, 13,\n",
       "        7,  5,  1,  7,  7,  7,  1, 10,  7,  0,  1,  1,  1, 13,  0,  1,  1,\n",
       "        1,  1, 13,  1,  4,  5, 10,  1,  1,  5,  5,  1,  1, 12,  1,  1, 10,\n",
       "       10,  5, 10,  4,  1,  5, 10,  7, 10,  1,  1,  5,  0,  1,  4,  5,  1,\n",
       "        5,  7,  7,  1,  7,  0,  1,  1,  4,  0,  1,  1,  0, 10,  1, 10,  1,\n",
       "        4, 10,  0,  1, 10,  4, 13, 10,  1,  1,  5,  1, 10,  7,  5,  1,  4,\n",
       "       13,  1,  1,  5,  5, 10,  7,  1,  1,  1, 10, 10,  0,  1, 10,  1, 12,\n",
       "        5,  1,  1,  1, 10,  0,  1, 10,  5,  1,  1,  1,  5,  1,  1,  1,  1,\n",
       "       10,  7,  5, 10,  4,  1,  7,  1,  1, 10, 10, 10, 13, 10, 10,  1, 10,\n",
       "        1,  1,  4,  1, 10,  1,  7, 10,  5,  5, 10,  7,  7, 10,  0,  1,  0,\n",
       "        1,  4, 13, 10, 10,  0, 10,  5,  1,  1,  1,  1, 11,  1,  4,  1,  1,\n",
       "        1,  1,  1,  1,  5, 10,  1,  1,  7,  5,  5, 10,  4,  1,  1,  5,  1,\n",
       "        5,  1,  1,  1,  5,  0,  1,  1,  4,  1,  7,  5,  1,  5,  1,  1,  1,\n",
       "        1,  1,  1,  7,  1,  1,  1, 10,  1,  1,  7,  5,  1,  5,  0,  1,  5,\n",
       "        1, 10,  1,  4,  1,  1,  4,  0, 10,  0,  5,  7,  1,  1,  4,  1,  1,\n",
       "        5,  1,  1,  1, 11, 11, 10,  1,  1,  5,  7,  1,  1,  1,  1,  1,  1,\n",
       "        5,  1, 10,  7,  1,  1,  1,  5,  5,  1,  4,  5,  1,  1,  4, 10,  1,\n",
       "        0, 10, 13, 13, 13,  7, 10,  7,  5,  1,  1, 11, 11,  4,  1, 10,  1,\n",
       "        1,  1, 10, 10,  1,  1,  1,  7, 13, 10, 10,  5, 10,  7, 13,  1,  4,\n",
       "        7,  1,  7,  1,  1,  1,  1,  1,  1,  1,  1,  5, 11,  4,  1,  1,  1,\n",
       "        1,  4, 11, 11,  1,  1, 10,  1, 13, 10, 10,  4,  5,  0,  5, 10,  5,\n",
       "       10, 10,  5, 10,  4, 10,  5, 10,  5,  4,  5,  5,  7, 10,  5, 10, 10,\n",
       "       10,  5, 10,  1,  1,  4,  1,  1,  1,  4,  7,  4,  1, 10,  4,  1,  4,\n",
       "        1,  1,  5,  1,  7,  1, 10, 13,  5,  1,  1,  1,  1,  7,  1,  7,  5,\n",
       "        1,  5,  1, 10,  1,  5,  1,  1,  1,  1,  0, 10, 10,  1,  1,  5,  0,\n",
       "        1,  7, 10, 13, 10, 10,  5,  1,  7, 10, 10,  5,  1,  1,  0,  5,  5,\n",
       "       10,  5,  5, 10,  1, 10,  7,  4,  7,  0, 10,  5,  1,  7,  1, 10, 10,\n",
       "        1, 10, 10, 10,  1,  7,  1,  1,  0,  5,  1, 10,  1,  1, 13,  4, 13,\n",
       "        0,  1, 10,  4,  1,  1,  0,  4,  5, 10,  4,  1,  1,  4,  4,  5, 10,\n",
       "        5,  1, 10,  7,  1,  7,  1,  7,  1,  1,  1,  1, 10,  5,  1, 10, 10,\n",
       "       10,  4,  7, 10,  5,  5,  5,  7,  7,  4,  7,  1,  4,  4,  1,  1,  1,\n",
       "       10,  1, 10,  1,  5,  1,  1,  1,  1,  1,  4,  7,  7,  1,  1,  1, 10,\n",
       "        1, 10,  4,  4,  5, 13,  7,  0,  1,  1, 13, 13, 13,  1,  1,  5,  1,\n",
       "        1, 13, 10, 13,  5, 10,  4, 13, 13, 13,  1,  1,  1,  1,  7,  1,  1,\n",
       "        1,  1,  4,  1,  1,  4,  5,  1,  1,  1,  1,  1,  1,  1,  1, 10, 13,\n",
       "        1, 10,  5,  1, 13, 13,  0, 13,  5,  1,  1,  5,  0,  1,  4, 10,  0,\n",
       "        1, 10,  5,  4,  4,  1, 10, 10,  7,  1, 10,  1,  5,  1, 10, 10,  7,\n",
       "       10,  5, 10, 10,  7,  1,  5,  5,  1, 10,  1,  1,  5,  7, 10,  1,  1,\n",
       "        1, 13,  0,  1,  1,  1,  7,  5,  1, 10,  7, 10,  4,  1,  1,  1,  1,\n",
       "        0,  1,  7,  1,  0,  0,  4, 10,  1, 11,  1,  1,  4,  1, 13,  4,  1,\n",
       "        1,  7,  0, 10,  5,  7,  5, 10,  1, 10,  7,  1,  1,  1,  1,  1,  1,\n",
       "        1,  0,  7,  1,  1,  5,  1,  0,  1,  1,  0,  1, 10, 10,  1,  1,  1,\n",
       "        0,  1,  5,  0,  7,  1,  1,  1,  1,  7,  4,  1, 10,  1,  1,  4])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=model.labels_\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "15cc1b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.06584605e-02, 1.99347051e-01, 7.37404624e-13, 7.37404624e-13,\n",
       "       1.11315182e-01, 4.15532431e-01, 4.09569377e-04, 7.08484080e-03,\n",
       "       7.37404624e-13, 7.37404624e-13, 2.40846331e-01, 3.08804406e-03,\n",
       "       5.26521302e-03, 6.45287780e-03])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.matrix_docs_topics_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d88acf",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c50ccdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "84ae16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast tweets to numpy array\n",
    "docs = my_df.clean.apply(lambda x: str(x).split()).to_numpy()\n",
    "\n",
    "# create dictionary of all words in all documents\n",
    "dictionary = gensim.corpora.Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1f660379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "61c34e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = btm.get_top_topic_words(model, words_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "38ae3d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>always</td>\n",
       "      <td>white</td>\n",
       "      <td>zomato</td>\n",
       "      <td>zomato</td>\n",
       "      <td>woman</td>\n",
       "      <td>muslim</td>\n",
       "      <td>courage</td>\n",
       "      <td>great</td>\n",
       "      <td>zomato</td>\n",
       "      <td>zomato</td>\n",
       "      <td>amp</td>\n",
       "      <td>nigga</td>\n",
       "      <td>wilder</td>\n",
       "      <td>cpac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anyone</td>\n",
       "      <td>people</td>\n",
       "      <td>fight</td>\n",
       "      <td>fight</td>\n",
       "      <td>amp</td>\n",
       "      <td>year</td>\n",
       "      <td>union</td>\n",
       "      <td>even</td>\n",
       "      <td>fight</td>\n",
       "      <td>fight</td>\n",
       "      <td>left</td>\n",
       "      <td>lil</td>\n",
       "      <td>maharashtra</td>\n",
       "      <td>trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>know</td>\n",
       "      <td>get</td>\n",
       "      <td>fire</td>\n",
       "      <td>fire</td>\n",
       "      <td>sharia</td>\n",
       "      <td>new</td>\n",
       "      <td>major</td>\n",
       "      <td>soral</td>\n",
       "      <td>fire</td>\n",
       "      <td>fire</td>\n",
       "      <td>biden</td>\n",
       "      <td>poverty</td>\n",
       "      <td>shivgaan</td>\n",
       "      <td>must</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>communism</td>\n",
       "      <td>black</td>\n",
       "      <td>fine</td>\n",
       "      <td>fine</td>\n",
       "      <td>let</td>\n",
       "      <td>terrorist</td>\n",
       "      <td>catherine</td>\n",
       "      <td>one</td>\n",
       "      <td>fine</td>\n",
       "      <td>fine</td>\n",
       "      <td>attack</td>\n",
       "      <td>percent</td>\n",
       "      <td>birth</td>\n",
       "      <td>lincoln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>national</td>\n",
       "      <td>say</td>\n",
       "      <td>find</td>\n",
       "      <td>find</td>\n",
       "      <td>hijab</td>\n",
       "      <td>migrant</td>\n",
       "      <td>something</td>\n",
       "      <td>someone</td>\n",
       "      <td>find</td>\n",
       "      <td>find</td>\n",
       "      <td>wing</td>\n",
       "      <td>shame</td>\n",
       "      <td>maharaj</td>\n",
       "      <td>president</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>want</td>\n",
       "      <td>anti</td>\n",
       "      <td>finally</td>\n",
       "      <td>finally</td>\n",
       "      <td>islamic</td>\n",
       "      <td>twitter</td>\n",
       "      <td>anyrtgingh</td>\n",
       "      <td>big</td>\n",
       "      <td>finally</td>\n",
       "      <td>finally</td>\n",
       "      <td>video</td>\n",
       "      <td>andrew</td>\n",
       "      <td>round</td>\n",
       "      <td>republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>social</td>\n",
       "      <td>one</td>\n",
       "      <td>final</td>\n",
       "      <td>final</td>\n",
       "      <td>cair</td>\n",
       "      <td>attack</td>\n",
       "      <td>salutation</td>\n",
       "      <td>world</td>\n",
       "      <td>final</td>\n",
       "      <td>final</td>\n",
       "      <td>warn</td>\n",
       "      <td>release</td>\n",
       "      <td>shivaji</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>firearm</td>\n",
       "      <td>go</td>\n",
       "      <td>filth</td>\n",
       "      <td>filth</td>\n",
       "      <td>one</td>\n",
       "      <td>france</td>\n",
       "      <td>infect</td>\n",
       "      <td>reset</td>\n",
       "      <td>filth</td>\n",
       "      <td>filth</td>\n",
       "      <td>border</td>\n",
       "      <td>bail</td>\n",
       "      <td>umbrella</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>come</td>\n",
       "      <td>like</td>\n",
       "      <td>film</td>\n",
       "      <td>film</td>\n",
       "      <td>state</td>\n",
       "      <td>illegal</td>\n",
       "      <td>visit</td>\n",
       "      <td>play</td>\n",
       "      <td>film</td>\n",
       "      <td>film</td>\n",
       "      <td>break</td>\n",
       "      <td>rise</td>\n",
       "      <td>dutch</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>major</td>\n",
       "      <td>want</td>\n",
       "      <td>file</td>\n",
       "      <td>file</td>\n",
       "      <td>omar</td>\n",
       "      <td>school</td>\n",
       "      <td>music</td>\n",
       "      <td>alain</td>\n",
       "      <td>file</td>\n",
       "      <td>file</td>\n",
       "      <td>germany</td>\n",
       "      <td>hut</td>\n",
       "      <td>across</td>\n",
       "      <td>national</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic0  topic1   topic2   topic3   topic4     topic5      topic6   topic7   topic8   topic9  topic10  topic11      topic12     topic13\n",
       "0     always   white   zomato   zomato    woman     muslim     courage    great   zomato   zomato      amp    nigga       wilder        cpac\n",
       "1     anyone  people    fight    fight      amp       year       union     even    fight    fight     left      lil  maharashtra       trump\n",
       "2       know     get     fire     fire   sharia        new       major    soral     fire     fire    biden  poverty     shivgaan        must\n",
       "3  communism   black     fine     fine      let  terrorist   catherine      one     fine     fine   attack  percent        birth     lincoln\n",
       "4   national     say     find     find    hijab    migrant   something  someone     find     find     wing    shame      maharaj   president\n",
       "5       want    anti  finally  finally  islamic    twitter  anyrtgingh      big  finally  finally    video   andrew        round  republican\n",
       "6     social     one    final    final     cair     attack  salutation    world    final    final     warn  release      shivaji     project\n",
       "7    firearm      go    filth    filth      one     france      infect    reset    filth    filth   border     bail     umbrella        show\n",
       "8       come    like     film     film    state    illegal       visit     play     film     film    break     rise        dutch         yes\n",
       "9      major    want     file     file     omar     school       music    alain     file     file  germany      hut       across    national"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5f9ebad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['always', 'anyone', 'know', 'communism', 'national', 'want',\n",
       "        'social', 'firearm', 'come', 'major'],\n",
       "       ['white', 'people', 'get', 'black', 'say', 'anti', 'one', 'go',\n",
       "        'like', 'want'],\n",
       "       ['zomato', 'fight', 'fire', 'fine', 'find', 'finally', 'final',\n",
       "        'filth', 'film', 'file'],\n",
       "       ['zomato', 'fight', 'fire', 'fine', 'find', 'finally', 'final',\n",
       "        'filth', 'film', 'file'],\n",
       "       ['woman', 'amp', 'sharia', 'let', 'hijab', 'islamic', 'cair',\n",
       "        'one', 'state', 'omar'],\n",
       "       ['muslim', 'year', 'new', 'terrorist', 'migrant', 'twitter',\n",
       "        'attack', 'france', 'illegal', 'school'],\n",
       "       ['courage', 'union', 'major', 'catherine', 'something',\n",
       "        'anyrtgingh', 'salutation', 'infect', 'visit', 'music'],\n",
       "       ['great', 'even', 'soral', 'one', 'someone', 'big', 'world',\n",
       "        'reset', 'play', 'alain'],\n",
       "       ['zomato', 'fight', 'fire', 'fine', 'find', 'finally', 'final',\n",
       "        'filth', 'film', 'file'],\n",
       "       ['zomato', 'fight', 'fire', 'fine', 'find', 'finally', 'final',\n",
       "        'filth', 'film', 'file'],\n",
       "       ['amp', 'left', 'biden', 'attack', 'wing', 'video', 'warn',\n",
       "        'border', 'break', 'germany'],\n",
       "       ['nigga', 'lil', 'poverty', 'percent', 'shame', 'andrew',\n",
       "        'release', 'bail', 'rise', 'hut'],\n",
       "       ['wilder', 'maharashtra', 'shivgaan', 'birth', 'maharaj', 'round',\n",
       "        'shivaji', 'umbrella', 'dutch', 'across'],\n",
       "       ['cpac', 'trump', 'must', 'lincoln', 'president', 'republican',\n",
       "        'project', 'show', 'yes', 'national']], dtype=object)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words.T.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "11730d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_gsdmm = CoherenceModel(topics=top_words.T.to_numpy(), \n",
    "                          dictionary=dictionary, \n",
    "                          corpus=bow_corpus,\n",
    "                          texts=docs, \n",
    "                          coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "22452d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = cm_gsdmm.get_coherence()  # get coherence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ea66092c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44762523099956014"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
